import pandas as pd
import sys, getopt, os, tarfile, fnmatch, csv, time
from natsort import natsorted, ns

# To calculate script execution time
start_time = time.time()

#Print version numbers of Python, Pandas and CSV modules installed on the host machine
print('Python Version installed '+ sys.version)
print('Pandas Version installed '+ pd.__version__)
print('CSV Version installed '+ csv.__version__)

# Relative or Absolute path to the directory which contains the reports generated by the certified Red Hat Cost Management Metrics Operator
inputDirName = sys.argv[1]
#print('Provided Directory Name: '+inputDirName)

# Step 1 - Read the contents of the provided directory and sort them in natural order
entries = natsorted(os.listdir(inputDirName), key=lambda y: y.lower())

# Create an empty Data Frame to store the final data after iterating through the directories
finalDataFrame = pd.DataFrame()

#""""
# Step 2 - Logic to extract all the reports generated for a particular month
# Iterate through the list of directories
for entry in entries:

    # Store the absolute path of the file
    fileName = inputDirName + entry

    # Check if it is a file and NOT a directory
    isFile = os.path.isfile(fileName)

    if isFile:

        # Check if it is a TAR file
        if tarfile.is_tarfile(fileName):
            #print(fileName)

            #Extract the contents of the TAR file
            file = tarfile.open(fileName)
            file.extractall(fileName[:-7])
            file.close()
#"""

#"""
# Step 3 - Logic to look at the reports generated every 6 hours
# Iterate through the list of directories
for entry in entries:
    
    # Store the absolute path of the file
    dirName = inputDirName + entry

    # Check if it is a file and NOT a directory
    isDir = os.path.isdir(dirName)
    
    if isDir:
        #print('Directory Name : ' + dirName)
        
        # Read the contents of the provided directory
        fileList = os.listdir(dirName)

        # 'node_capacity_cpu_cores' information is currently present in a CSV file called _openshift_usage_report.2.csv
        pattern = '*_openshift_usage_report.*.csv'

        # Iterate through the list of files in the directory
        for file in fileList:

            # Check if the file name matches the pattern
            if fnmatch.fnmatch(file, pattern):
                fileName = dirName + '/' + file

                # Ingest the information from the CSV file
                csvData = pd.read_csv(fileName)
                #print('Usage File : ' + fileName)

                # Get the list of column names from the CSV file
                columnsList = list(csvData.columns)
                
                # Check if the column list contains the attribute 'node_capacity_cpu_cores'
                if('node_capacity_cpu_cores' in columnsList):
                    #print('Axes : '+ str(csvData.axes))
                    #print('File '+fileName+' has the column "node_capacity_cpu_cores"')

                    # The current CSV file contains more than 19 columns. We only need 4 columns.
                    subsetData = csvData[['interval_start', 'interval_end', 'node', 'node_capacity_cpu_cores']]
                    #print(subsetData.shape)
                    
                    # Concat the data to the empty DataFrame created at the beginning, since we are iterating through the list of files, 
                    finalDataFrame = pd.concat([finalDataFrame, subsetData], axis = 0)

# Step 4 - Group the data to put into a XLSX document
# Group the information for each 1 hour interval to highlight the vCPU cores used by the nodes in the OpenShift cluster
vCpuUsed = finalDataFrame.groupby(['interval_start', 'interval_end', 'node'])['node_capacity_cpu_cores'].max()
#print(vCpuUsed)

# Dump the information into a XLSX document
vCpuUsed.to_excel('vcpu_count.xlsx')

print("--- Script execution time: %s seconds ---" % round(time.time() - start_time))
#"""